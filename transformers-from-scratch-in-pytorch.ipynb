{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformers Paper Implementation from scratch in Pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"resources/transformer.png\" alt=\"Description\" width=\"500\" height=\"500\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-11T12:07:08.113072Z",
     "iopub.status.busy": "2025-02-11T12:07:08.112768Z",
     "iopub.status.idle": "2025-02-11T12:07:15.125417Z",
     "shell.execute_reply": "2025-02-11T12:07:15.122937Z",
     "shell.execute_reply.started": "2025-02-11T12:07:08.113046Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchtext==0.1.1\n",
      "  Downloading torchtext-0.1.1-py3-none-any.whl.metadata (386 bytes)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext==0.1.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext==0.1.1) (4.67.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.1.1) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.1.1) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.1.1) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext==0.1.1) (2025.1.31)\n",
      "Downloading torchtext-0.1.1-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: torchtext\n",
      "Successfully installed torchtext-0.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext==0.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:07:16.633281Z",
     "iopub.status.busy": "2025-02-11T12:07:16.632908Z",
     "iopub.status.idle": "2025-02-11T12:07:21.459739Z",
     "shell.execute_reply": "2025-02-11T12:07:21.458327Z",
     "shell.execute_reply.started": "2025-02-11T12:07:16.633251Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:07:24.649168Z",
     "iopub.status.busy": "2025-02-11T12:07:24.648501Z",
     "iopub.status.idle": "2025-02-11T12:07:24.656746Z",
     "shell.execute_reply": "2025-02-11T12:07:24.655249Z",
     "shell.execute_reply.started": "2025-02-11T12:07:24.649121Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math,re\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torchtext\n",
    "import torch.optim as optim\n",
    "warnings.simplefilter(\"ignore\")\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Token Embedding in Transformers  \n",
    "\n",
    "- This `Embedding` class converts token indices into meaningful dense vectors, making it easier for the transformer to understand text.  \n",
    "- Think of it as a lookup table where each word (or subword) gets its own unique representation, which the model learns and refines during training.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:07:26.799662Z",
     "iopub.status.busy": "2025-02-11T12:07:26.799133Z",
     "iopub.status.idle": "2025-02-11T12:07:26.809376Z",
     "shell.execute_reply": "2025-02-11T12:07:26.807048Z",
     "shell.execute_reply.started": "2025-02-11T12:07:26.799620Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self,vocab_size,d_model): \n",
    "        super().__init__()\n",
    "        self.embedding_layer = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embedding_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding in Transformers  \n",
    "  \n",
    "- This implementation uses sine and cosine functions at different frequencies to create unique position-based patterns, helping the model distinguish word positions.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:07:33.809446Z",
     "iopub.status.busy": "2025-02-11T12:07:33.809056Z",
     "iopub.status.idle": "2025-02-11T12:07:33.816859Z",
     "shell.execute_reply": "2025-02-11T12:07:33.815744Z",
     "shell.execute_reply.started": "2025-02-11T12:07:33.809372Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,d_model, max_len): \n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        pos_matrix = torch.zeros((max_len, d_model))\n",
    "        \n",
    "        for pos in range(max_len): \n",
    "            for i in range(0, d_model//2):\n",
    "                pos_matrix[pos,i*2] = math.sin(pos/10000**((2*i)/d_model))\n",
    "                pos_matrix[pos,i*2+1] = math.cos(pos/10000**((2*i+1)/d_model))\n",
    "\n",
    "        pos_matrix = pos_matrix.unsqueeze(0) #batch_size, max_seq_len, d_model\n",
    "        self.register_buffer('pos_matrix', pos_matrix)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # print(f'In PosEnc.forward->x.shape: {x.shape}')\n",
    "        x = x*math.sqrt(self.d_model)\n",
    "        seq_len = x.shape[1] #TODO: \n",
    "        return x + self.pos_matrix[:,:seq_len] \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Heads Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:07:37.850694Z",
     "iopub.status.busy": "2025-02-11T12:07:37.849907Z",
     "iopub.status.idle": "2025-02-11T12:07:37.862907Z",
     "shell.execute_reply": "2025-02-11T12:07:37.861460Z",
     "shell.execute_reply.started": "2025-02-11T12:07:37.850631Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,d_model,num_heads): \n",
    "        super().__init__()\n",
    "        assert d_model%num_heads == 0 , \"d_model must be divisible by num_heads\"\n",
    "\n",
    "        self.d_key = d_model // num_heads\n",
    "\n",
    "        # matrices for all heads are combined\n",
    "        self.Q = nn.Linear(d_model,d_model, bias=False) \n",
    "        self.K = nn.Linear(d_model,d_model, bias=False)\n",
    "        self.V = nn.Linear(d_model,d_model, bias=False)\n",
    "        self.O = nn.Linear(d_model,d_model, bias=True)\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        \n",
    "    def forward(self,Q,K,V,mask = None):\n",
    "        batch_size = K.shape[0]\n",
    "        len_seq = K.shape[1]\n",
    "        len_query = Q.shape[1]\n",
    "\n",
    "        Q = self.Q(Q)\n",
    "        K = self.K(K)\n",
    "        V = self.V(V)\n",
    "\n",
    "        Q = Q.view(batch_size, len_query, self.num_heads, self.d_key)\n",
    "        K = K.view(batch_size, len_seq, self.num_heads, self.d_key)\n",
    "        V = V.view(batch_size, len_seq, self.num_heads, self.d_key)\n",
    "\n",
    "        # seperate out heads, heads as channel dimension\n",
    "        K = K.transpose(1,2) # batch_size, num_heads, len_seq, d_key\n",
    "        Q = Q.transpose(1,2)\n",
    "        V = V.transpose(1,2) \n",
    "\n",
    "        # K.T\n",
    "        K = K.transpose(-1,-2) #batch_size, num_heads, d_key, len_seq\n",
    "        #TODO: Understand the multiplication here\n",
    "        # Q*K.T\n",
    "        raw_attention_matrix = torch.matmul(Q,K)\n",
    "        if mask is not None: \n",
    "            raw_attention_matrix = raw_attention_matrix.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        \n",
    "        raw_attention_matrix = raw_attention_matrix/math.sqrt(self.d_key)\n",
    "        scores = F.softmax(raw_attention_matrix, dim=-1) # seq_len * seq_len\n",
    "        \n",
    "        new_embeddings = torch.matmul(scores,V) # batch_size,num_heads,seq_len,d_model\n",
    "        new_embeddings = new_embeddings.transpose(1,2) # batch_size,seq_len,num_heads,d_key\n",
    "        new_embeddings = new_embeddings.contiguous().view(batch_size,len_seq,self.d_model)\n",
    "        \n",
    "        return self.O(new_embeddings) # batch_size, seq_len, d_model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:07:42.583583Z",
     "iopub.status.busy": "2025-02-11T12:07:42.583168Z",
     "iopub.status.idle": "2025-02-11T12:07:42.589690Z",
     "shell.execute_reply": "2025-02-11T12:07:42.588507Z",
     "shell.execute_reply.started": "2025-02-11T12:07:42.583552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module): \n",
    "    def __init__(self, d_model, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(d_model, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:07:44.238601Z",
     "iopub.status.busy": "2025-02-11T12:07:44.238256Z",
     "iopub.status.idle": "2025-02-11T12:07:44.245632Z",
     "shell.execute_reply": "2025-02-11T12:07:44.244086Z",
     "shell.execute_reply.started": "2025-02-11T12:07:44.238574Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module): \n",
    "    def __init__(self, d_model,num_heads,hidden_dim, dropout_rate = 0.2):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(d_model, num_heads)\n",
    "        self.drop1 = nn.Dropout(dropout_rate)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.ff = FeedForward(d_model,hidden_dim)\n",
    "        self.drop2 = nn.Dropout(dropout_rate)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "\n",
    "    def forward(self,x,mask=None):  # dropout is added before adding res \n",
    "        x = self.norm1(x+ self.drop1(self.att(x,x,x,mask)))\n",
    "        x = self.norm2(x+ self.drop2(self.ff(x)))\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:07:52.308617Z",
     "iopub.status.busy": "2025-02-11T12:07:52.308155Z",
     "iopub.status.idle": "2025-02-11T12:07:52.316375Z",
     "shell.execute_reply": "2025-02-11T12:07:52.314977Z",
     "shell.execute_reply.started": "2025-02-11T12:07:52.308584Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module): \n",
    "\n",
    "    def __init__(self,d_model, num_heads, hidden_dim, dropout=0.2): \n",
    "        super().__init__()\n",
    "        self.self_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.cross_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.feed_forward = FeedForward(d_model,hidden_dim)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.drop1 = nn.Dropout(dropout)\n",
    "        self.drop2 = nn.Dropout(dropout)\n",
    "        self.drop3 = nn.Dropout(dropout)\n",
    "\n",
    "\n",
    "    def forward(self,x,attention_output,source_mask,target_mask): \n",
    "        x = self.norm1(x + self.drop1(self.self_attention(x,x,x,target_mask)))\n",
    "        x = self.norm2(x + self.drop2(self.cross_attention(x,attention_output,attention_output,source_mask)))\n",
    "        x = self.norm3(x + self.drop3(self.feed_forward(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's pack it up into Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:07:54.948420Z",
     "iopub.status.busy": "2025-02-11T12:07:54.947959Z",
     "iopub.status.idle": "2025-02-11T12:07:54.958762Z",
     "shell.execute_reply": "2025-02-11T12:07:54.957488Z",
     "shell.execute_reply.started": "2025-02-11T12:07:54.948356Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,\n",
    "                 d_model,\n",
    "                 num_layers,\n",
    "                 num_heads,\n",
    "                 hidden_dim,\n",
    "                 max_len,\n",
    "                 source_vocab_size,\n",
    "                 target_vocab_size,\n",
    "                 dropout=0.2):\n",
    "        super().__init__()\n",
    "        # Embeddings\n",
    "        self.encoder_emb = nn.Embedding(source_vocab_size, d_model)\n",
    "        self.decoder_emb = nn.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_enc = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Encoder and decoder layers\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, hidden_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, hidden_dim, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "        \n",
    "        # Final linear layer mapping to target vocab size\n",
    "        self.fc = nn.Linear(d_model, target_vocab_size)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        # Example mask generation (adjust as needed)\n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool()\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, source, target):\n",
    "        # Generate masks\n",
    "        source_mask, target_mask = self.generate_mask(source, target)\n",
    "        \n",
    "        # Apply embedding and positional encoding with dropout\n",
    "        source_emb = self.dropout(self.pos_enc(self.encoder_emb(source)))\n",
    "        target_emb = self.dropout(self.pos_enc(self.decoder_emb(target)))\n",
    "\n",
    "        # Pass through encoder layers\n",
    "        enc_output = source_emb\n",
    "        for layer in self.encoder_layers:\n",
    "            enc_output = layer(enc_output, source_mask)\n",
    "\n",
    "        # Pass through decoder layers\n",
    "        dec_output = target_emb\n",
    "        for layer in self.decoder_layers:\n",
    "            dec_output = layer(dec_output, enc_output, source_mask, target_mask)\n",
    "\n",
    "        # Final linear mapping\n",
    "        return self.fc(dec_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:11:43.123268Z",
     "iopub.status.busy": "2025-02-11T12:11:43.122807Z",
     "iopub.status.idle": "2025-02-11T12:11:43.128653Z",
     "shell.execute_reply": "2025-02-11T12:11:43.127328Z",
     "shell.execute_reply.started": "2025-02-11T12:11:43.123233Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "d_model=512\n",
    "num_layers=6\n",
    "num_heads=4\n",
    "hidden_dim=2048\n",
    "MAX_LEN = 100\n",
    "source_vocab_size=1000\n",
    "target_vocab_size=1000\n",
    "dropout=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T12:11:43.927604Z",
     "iopub.status.busy": "2025-02-11T12:11:43.927079Z",
     "iopub.status.idle": "2025-02-11T12:11:44.940558Z",
     "shell.execute_reply": "2025-02-11T12:11:44.938772Z",
     "shell.execute_reply.started": "2025-02-11T12:11:43.927569Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "transformer = Transformer(d_model=d_model,\n",
    "                          num_layers=num_layers,\n",
    "                          num_heads=num_heads,\n",
    "                          hidden_dim=hidden_dim,\n",
    "                          max_len = MAX_LEN,\n",
    "                          source_vocab_size=source_vocab_size,\n",
    "                          target_vocab_size=target_vocab_size,\n",
    "                          dropout=dropout\n",
    "                         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simulate some random tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-11T11:47:04.245699Z",
     "iopub.status.busy": "2025-02-11T11:47:04.245362Z",
     "iopub.status.idle": "2025-02-11T11:47:04.251343Z",
     "shell.execute_reply": "2025-02-11T11:47:04.250206Z",
     "shell.execute_reply.started": "2025-02-11T11:47:04.245671Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Generate random sample data\n",
    "source_data = torch.randint(1, 1000, (64, max_seq_length))  # (batch_size, seq_length)\n",
    "target_data = torch.randint(1, 1000, (64, max_seq_length))  # (batch_size, seq_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "# optimizer = optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# transformer.train()\n",
    "\n",
    "# for epoch in range(100):\n",
    "#     optimizer.zero_grad()\n",
    "#     output = transformer(source_data, target_data[:, :-1])\n",
    "#     loss = criterion(output.contiguous().view(-1, target_vocab_size), target_data[:, 1:].contiguous().view(-1))\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 30886,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
